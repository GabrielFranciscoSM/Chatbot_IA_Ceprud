services:
    vllm-openai:
        container_name: my-vllm-service
        deploy:
            # limits: 
            #     memory: 4G
            #     cpus: '1.0'
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          device_ids: ['0']
                          capabilities:
                              - gpu
        volumes:
            - "./app/ml/models:/models"
        environment:
            HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
            #VLLM_LOGGING_LEVEL: DEBUG
        ports:
            - 8000:8000
        ipc: host
        image: docker.io/vllm/vllm-openai:latest
        command: [
            "--model", "/models/Sreenington--Phi-3-mini-4k-instruct-AWQ",
            #"--model", "/models/ibm-granite--granite-3.3-2b-instruct",
            "--max-num-seqs", "8",
            "--max-model-len", "1000",
            #"--max-batch-size", "8",
            "--quantization", "awq_marlin",       # <-- ADD: Enable 4-bit quantization
            "--gpu-memory-utilization", "0.7",
            #"--enable-lora",
            #"--max-loras", "1", # Set to the number of adapters you'll use
            #"--lora-modules", "metaheuristicas=/models/metaheuristicas-TinyLlama-qlora",
            "--port", "8000",
            
            "--enable-auto-tool-choice",
            "--tool-call-parser", "phi4_mini_json", 
            "--chat-template", "{{ bos_token }}{% for message in messages %}{% if (message['role'] in ['user', 'system']) %}{{'<|user|>' + '\n' + message['content'] + '<|end|>' + '\n' + '<|assistant|>' + '\n'}}{% elif message['role'] == 'assistant' %}{{message['content'] + '<|end|>' + '\n'}}{% endif %}{% endfor %}",
            ]             
    vllm-openai-embeddings:
        container_name: my-embedding-service
        deploy:
            # limits: 
            #     memory: 4G
            #     cpus: '1.0'
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          device_ids: ['0']
                          capabilities:
                              - gpu
        volumes:
            - "./app/ml/models:/models"
        environment:
            HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
            #VLLM_LOGGING_LEVEL: DEBUG
        ports:
            - 8001:8001
        ipc: host
        image: docker.io/vllm/vllm-openai:latest
        command: --model /models/BAAI--bge-m3 --port 8001 --max-num-seqs 8 --gpu-memory-utilization 0.15 --max_model_len=3000
    chatbot:
        container_name: my-chatbot-app
        build:
            context: .
            dockerfile: Containerfile
        ports:
            - 5001:5001
        deploy:
            # limits: 
            #     memory: 2G
            #     cpus: '1.0'
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          # You can specify a particular GPU or all available GPUs
                          # For example, to use the first GPU:
                          device_ids: ['0']
                          # To use all available GPUs, you can use:
                          # count: all
                          capabilities:
                              - gpu
        environment:
            HF_HUB_DISABLE_SYMLINKS_WARNING: 1
        volumes:
        #- ./app:/app
        - ./.env:/app/.env
        - ./unitTests:/unitTests
        - ./pytest.ini:/pytest.ini


        restart: unless-stopped

        depends_on:
            - vllm-openai
            - vllm-openai-embeddings