services:
    vllm-openai:
        container_name: my-vllm-service
        deploy:
            # limits: 
            #     memory: 4G
            #     cpus: '1.0'
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          device_ids: ['0']
                          capabilities:
                              - gpu
        volumes:
            - "./models:/models"
        environment:
            HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
            #VLLM_LOGGING_LEVEL: DEBUG
        ports:
            - 8000:8000
        ipc: host
        image: docker.io/vllm/vllm-openai:latest
        command: [
            "--model", "/models/Sreenington--Phi-3-mini-4k-instruct-AWQ",
            #"--model", "/models/ibm-granite--granite-3.3-2b-instruct",
            "--max-num-seqs", "8",
            "--max-model-len", "1000",
            #"--max-batch-size", "8",
            "--quantization", "awq_marlin",       # <-- ADD: Enable 4-bit quantization
            "--gpu-memory-utilization", "0.7",
            #"--enable-lora",
            #"--max-loras", "1", # Set to the number of adapters you'll use
            #"--lora-modules", "metaheuristicas=/models/metaheuristicas-TinyLlama-qlora",
            "--port", "8000",
            
            "--enable-auto-tool-choice",
            "--tool-call-parser", "phi4_mini_json", 
            "--chat-template", "{{ bos_token }}{% for message in messages %}{% if (message['role'] in ['user', 'system']) %}{{'<|user|>' + '\n' + message['content'] + '<|end|>' + '\n' + '<|assistant|>' + '\n'}}{% elif message['role'] == 'assistant' %}{{message['content'] + '<|end|>' + '\n'}}{% endif %}{% endfor %}",
            ]             
    vllm-openai-embeddings:
        container_name: my-embedding-service
        deploy:
            # limits: 
            #     memory: 4G
            #     cpus: '1.0'
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          device_ids: ['0']
                          capabilities:
                              - gpu
        volumes:
            - "./models:/models"
        environment:
            HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
            #VLLM_LOGGING_LEVEL: DEBUG
        ports:
            - 8001:8001
        ipc: host
        image: docker.io/vllm/vllm-openai:latest
        command: --model /models/Qwen--Qwen3-Embedding-0.6B --port 8001 --max-num-seqs 64 --gpu-memory-utilization 0.15 --max_model_len=3000
    chatbot:
        container_name: my-chatbot-app
        build:
            context: .
            dockerfile: Containerfile
        ports:
            - 5001:5001
        deploy:
            # limits: 
            #     memory: 2G
            #     cpus: '1.0'
            resources:
                reservations:
                    devices:
                        - driver: nvidia
                          # You can specify a particular GPU or all available GPUs
                          # For example, to use the first GPU:
                          device_ids: ['0']
                          # To use all available GPUs, you can use:
                          # count: all
                          capabilities:
                              - gpu
        env_file:
            - .env
        environment:
            HF_HUB_DISABLE_SYMLINKS_WARNING: 1
            PYTHONPATH: /chatbot/app
        command: ["uvicorn", "app.app:app", "--host", "0.0.0.0", "--port", "5001"]
        volumes:
        # DEVELOPMENT: Uncomment next line for hot-reload (code changes without rebuild)
        # - ./app:/app  # Mount the app directory for live code updates
        #S- ./.env:/app/.env
        - ./unitTests:/chatbot/app/unitTests
        - ./pytest.ini:/chatbot/app/pytest.ini


        restart: unless-stopped

        depends_on:
            - vllm-openai
            - vllm-openai-embeddings