services:
    # LLM Service
    # vllm-openai:
    #     container_name: my-vllm-service
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       device_ids: ['0']
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - "./models:/models"
    #     environment:
    #         HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
    #     ports:
    #         - 8000:8000
    #     image: docker.io/vllm/vllm-openai:latest
    #     command: [
    #         "--model", "/models/Sreenington--Phi-3-mini-4k-instruct-AWQ",
    #         "--max-num-seqs", "8",
    #         "--max-model-len", "1000",
    #         "--quantization", "awq_marlin",
    #         "--gpu-memory-utilization", "0.7",
    #         "--port", "8000",
    #         "--enable-auto-tool-choice",
    #         "--tool-call-parser", "phi4_mini_json", 
    #         "--chat-template", "{{ bos_token }}{% for message in messages %}{% if (message['role'] in ['user', 'system']) %}{{'<|user|>' + '\n' + message['content'] + '<|end|>' + '\n' + '<|assistant|>' + '\n'}}{% elif message['role'] == 'assistant' %}{{message['content'] + '<|end|>' + '\n'}}{% endif %}{% endfor %}",
    #     ]
        
    #Embeddings Service
    # vllm-openai-embeddings:
    #     container_name: my-embedding-service
    #     deploy:
    #         resources:
    #             reservations:
    #                 devices:
    #                     - driver: nvidia
    #                       device_ids: ['0']
    #                       capabilities:
    #                           - gpu
    #     volumes:
    #         - "./models:/models"
    #     environment:
    #         HUGGING_FACE_HUB_TOKEN: ${HF_TOKEN}
    #     image: docker.io/vllm/vllm-openai:latest
    #     command: --model /models/Qwen--Qwen3-Embedding-0.6B --port 8001 --max-num-seqs 16 --gpu-memory-utilization 0.4 --max_model_len=3000

    # Ollama Service (CPU-based alternative for embeddings)
    ollama:
        container_name: chatbot-ollama
        image: docker.io/ollama/ollama:latest
        ports:
            - "11434:11434"
        volumes:
            - ollama_data:/root/.ollama
        environment:
            # Optimización: Permitir más paralelismo
            OLLAMA_NUM_PARALLEL: "4"  # Procesar 4 requests en paralelo
            OLLAMA_MAX_LOADED_MODELS: "2"
        restart: unless-stopped

    # MongoDB Database
    mongodb:
        container_name: chatbot-mongodb
        image: docker.io/mongo:8.0
        ports:
            - "27017:27017"
        environment:
            MONGO_INITDB_ROOT_USERNAME: admin
            MONGO_INITDB_ROOT_PASSWORD: password123
            MONGO_INITDB_DATABASE: chatbot_users
        volumes:
            - mongodb_data:/data/db
        restart: unless-stopped

    # User Data Service (MongoDB)
    user-service:
        container_name: chatbot-user-service
        build:
            context: ./mongo-service
            dockerfile: Dockerfile
        ports:
            - "8083:8083"
        environment:
            MONGODB_URL: mongodb://admin:password123@mongodb:27017
            MONGODB_DATABASE: chatbot_users
        depends_on:
            - mongodb
        restart: unless-stopped
    mongo-express:
        container_name: chatbot-mongo-express
        image: docker.io/mongo-express
        restart: always
        ports:
            - "8081:8081"
        environment:
            ME_CONFIG_MONGODB_URL: mongodb://admin:password123@mongodb:27017
            ME_CONFIG_BASICAUTH_ENABLED: true
            ME_CONFIG_BASICAUTH_USERNAME: mongoexpressuser
            ME_CONFIG_BASICAUTH_PASSWORD: mongoexpresspass
            MONGO_INITDB_DATABASE: chatbot_users
        depends_on:
            - mongodb

    # RAG Service
    rag-service:
        container_name: chatbot-rag-service
        build:
            context: ./rag-service
            dockerfile: Dockerfile
        ports:
            - "8082:8082"
        environment:
            BASE_CHROMA_PATH: /app/data/chroma
            VLLM_EMBEDDING_URL: http://vllm-openai-embeddings:8001
            EMBEDDING_MODEL_DIR: /models/Qwen--Qwen3-Embedding-0.6B
            # Ollama configuration (set USE_OLLAMA=true to use CPU-based embeddings)
            USE_OLLAMA: "true"
            OLLAMA_URL: http://ollama:11434
            OLLAMA_MODEL_NAME: nomic-embed-text
        volumes:
            - ./rag-service/data:/app/data/:z  # Guías docentes y archivos de asignaturas
            - rag_data:/app/data/chroma:z  # Volumen persistente para ChromaDB
        restart: unless-stopped
        depends_on:
            - ollama
        # depends_on:
        #     - vllm-openai-embeddings

    # Logging Service
    logging-service:
        container_name: chatbot-logging-service
        build:
            context: ./logging-service
            dockerfile: Dockerfile
        ports:
            - "8002:8002"
        environment:
            BASE_LOG_DIR: /app/logs
            MONGODB_URL: mongodb://admin:password123@mongodb:27017
            MONGODB_DATABASE: chatbot_logs
        volumes:
            - ./logs:/app/logs:z  # Mount logs directory (fallback CSV)
        depends_on:
            - mongodb
        restart: unless-stopped

    # Backend Service
    backend:
        container_name: chatbot-backend
        build:
            context: .
            dockerfile: Containerfile
        ports:
            - "8080:8080"
        env_file:
            - .env
        environment:
            HF_HUB_DISABLE_SYMLINKS_WARNING: 1
            PYTHONPATH: /chatbot/app
            RAG_SERVICE_URL: http://rag-service:8082
            LOGGING_SERVICE_URL: http://logging-service:8002
            USER_SERVICE_URL: http://user-service:8083
            MONGO_URI: mongodb://admin:password123@mongodb:27017
            MONGODB_DATABASE: chatbot_users
        command: ["uvicorn", "app.app:app", "--host", "0.0.0.0", "--port", "8080"]
        volumes:
            - ./unitTests:/chatbot/app/unitTests:z
        restart: unless-stopped
        depends_on:
            # - vllm-openai-embeddings
            - rag-service

    # Frontend Service
    frontend:
        container_name: chatbot-frontend
        build:
            context: ./frontend
            dockerfile: Dockerfile
        ports:
            - "8090:8090"
        environment:
            - NGINX_HOST=localhost
            - NGINX_PORT=8090
        depends_on:
            - backend
        restart: unless-stopped

volumes:
    rag_data:
        driver: local
    mongodb_data:
        driver: local
    ollama_data:
        driver: local
networks:
    default:
        name: chatbot-network

